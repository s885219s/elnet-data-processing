{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imutils import face_utils\n",
    "import numpy as np\n",
    "import imutils\n",
    "import dlib\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We have created 'not_found_list.txt' via the deteciton process, so we can just directly read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#512x512\n",
    "#256x256\n",
    "root = 'D:/share/new_dataset/new_human_power_reorder/dirl_new_face/512x512/'\n",
    "img_root = 'D:/share/new_dataset/new_human_power_reorder/dirl_new_face/512x512/HELEN/'\n",
    "save_folder = 'HELEN_predict_dlib'\n",
    "# dlib_root = 'D:/share/new_dataset/new_human_power_reorder/dirl_face/face_ssd_feature_points/dlib/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(root,save_folder,'not_found_list.txt'),'r') as nfl:\n",
    "    nfl = nfl.read()\n",
    "    nfl = nfl.split('\\n')[:-1]\n",
    "    nfl = [os.path.basename(n) for n in nfl]\n",
    "    nfl = np.array(nfl)\n",
    "print(len(nfl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ap_dict = {}\n",
    "mean_list = []\n",
    "with open(img_root+'fileList.txt','r') as t:\n",
    "    lines = t.read()\n",
    "    lines = lines.split('\\n')\n",
    "    lines = lines[:-1]\n",
    "    for line in lines:\n",
    "        fn = os.path.basename(line)\n",
    "        path = os.path.dirname(line)\n",
    "        txt_file = os.path.join(root,save_folder,line)[:-4]+'.txt'\n",
    "        if not fn in nfl:\n",
    "            ap = np.genfromtxt(txt_file,delimiter=',')\n",
    "            ap = ap[:,2:]\n",
    "            for n,p in enumerate(ap):\n",
    "                if not n in ap_dict:\n",
    "                    ap_dict[n] = list()\n",
    "                ap_dict[n].append(p)\n",
    "    for i in range(14):\n",
    "        np_ap = np.array(ap_dict[i])\n",
    "        mean_ap = np.around(np.mean(np_ap,0)).astype(np.int)\n",
    "        mean_list.append(mean_ap)\n",
    "    with open(os.path.join(root,save_folder,'mean_ap.txt'),'w+') as m:\n",
    "        for n,mean_ap in enumerate(mean_list):\n",
    "            m.write(str(n//7)+','+str(n%7)+','+str(mean_ap[0])+','+str(mean_ap[1])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_ap = np.genfromtxt(os.path.join(root,save_folder,'mean_ap.txt'),delimiter=',').astype(np.int)\n",
    "    \n",
    "with open(os.path.join(root,save_folder,'not_found_list.txt'),'r') as nfl:\n",
    "    nfl = nfl.read()\n",
    "    nfl = nfl.split('\\n')[:-1]\n",
    "    nfl = [n[:-4]+'.txt' for n in nfl]\n",
    "    for file in nfl:\n",
    "        txt = file[:-4]+'.txt'\n",
    "        with open(txt,'w+') as t:\n",
    "            for line in mean_ap:\n",
    "                t.write(str(line[0])+','+str(line[1])+','+str(line[2])+','+str(line[3])+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# openface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We have not created 'not_found_list.txt' via the deteciton process, \n",
    "#so we have to first check all the output txt which coord value is 0.\n",
    "#Then, create the 'not_cound_list.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = 'D:/share/new_dataset/new_human_power_reorder/dirl_new_face/256x256/'\n",
    "img_root = 'D:/share/new_dataset/new_human_power_reorder/dirl_new_face/256x256/300W/'\n",
    "save_folder = '300W_predict_openface/'\n",
    "# dlib_root = 'D:/share/new_dataset/new_human_power_reorder/dirl_face/face_ssd_feature_points/dlib/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for fn_list in os.walk(os.path.join(root,save_folder)):\n",
    "    for fn in fn_list[2]:\n",
    "        if fn.endswith('.txt'):\n",
    "            txt_name = os.path.join(fn_list[0],fn)\n",
    "            txt = np.genfromtxt(txt_name,delimiter=',')\n",
    "            if txt[0][2]== 0 and txt[0][3] == 0:\n",
    "                with open(os.path.join(root,save_folder,'not_found_list.txt'),'a+') as nfl:\n",
    "                    nfl.write(txt_name+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(root,save_folder,'not_found_list.txt'),'r') as nfl:\n",
    "    nfl = nfl.read()\n",
    "    nfl = nfl.split('\\n')[:-1]\n",
    "    nfl = [os.path.basename(n) for n in nfl]\n",
    "    nfl = np.array(nfl)\n",
    "print(len(nfl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ap_dict = {}\n",
    "mean_list = []\n",
    "\n",
    "with open(img_root+'fileList.txt','r') as t:\n",
    "    lines = t.read()\n",
    "    lines = lines.split('\\n')\n",
    "    lines = lines[:-1]\n",
    "    for line in lines:\n",
    "        fn = os.path.basename(line)\n",
    "        path = os.path.dirname(line)\n",
    "        txt_file = os.path.join(root,save_folder,line)[:-4]+'.txt'\n",
    "        if not fn in nfl:\n",
    "            ap = np.genfromtxt(txt_file,delimiter=',')\n",
    "            ap = ap[:,2:]\n",
    "            for n,p in enumerate(ap):\n",
    "                if not n in ap_dict:\n",
    "                    ap_dict[n] = list()\n",
    "                ap_dict[n].append(p)\n",
    "    for i in range(14):\n",
    "        np_ap = np.array(ap_dict[i])\n",
    "        mean_ap = np.around(np.mean(np_ap,0)).astype(np.int)\n",
    "        mean_list.append(mean_ap)\n",
    "    with open(os.path.join(root,save_folder,'mean_ap.txt'),'w+') as m:\n",
    "        for n,mean_ap in enumerate(mean_list):\n",
    "            m.write(str(n//7)+','+str(n%7)+','+str(mean_ap[0])+','+str(mean_ap[1])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_ap = np.genfromtxt(os.path.join(root,save_folder,'mean_ap.txt'),delimiter=',').astype(np.int)\n",
    "    \n",
    "with open(os.path.join(root,save_folder,'not_found_list.txt'),'r') as nfl:\n",
    "    nfl = nfl.read()\n",
    "    nfl = nfl.split('\\n')[:-1]\n",
    "    nfl = [n[:-4]+'.txt' for n in nfl]\n",
    "    for file in nfl:\n",
    "        txt = file[:-4]+'.txt'\n",
    "        with open(txt,'w+') as t:\n",
    "            for line in mean_ap:\n",
    "                t.write(str(line[0])+','+str(line[1])+','+str(line[2])+','+str(line[3])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
